{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the directory path of the notebook\n",
    "notebook_directory = os.getcwd()\n",
    "\n",
    "# Navigate one level up to reach the root directory\n",
    "root_directory = os.path.abspath(os.path.join(notebook_directory, os.pardir))\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_directory)\n",
    "\n",
    "src_directory = os.path.join(root_directory, \"src\")\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rulefit import RuleFit\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNet, Ridge\n",
    "from pyod.models.ecod import ECOD\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest, RandomForestRegressor, GradientBoostingRegressor, \\\n",
    "    StackingRegressor, AdaBoostRegressor, VotingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler, RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, Matern, WhiteKernel\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can import the preprocessing module and use its functions\n",
    "from preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U75fwOQGN_hR"
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "ecod = ECOD(contamination=0.01)\n",
    "knn_imputer = KNNImputer(n_neighbors=15, weights=\"uniform\")\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "selector = SelectKBest(score_func=f_regression, k=175)\n",
    "scaler_x = RobustScaler()\n",
    "\n",
    "\n",
    "# Instantiate Preprocessing object\n",
    "preprocessing = Preprocessing(imputer, ecod, knn_imputer, constant_filter, selector, scaler_x)\n",
    "X_train = Preprocessing.load_data(os.path.join(root_directory, \"data\", \"X_train.csv\"))\n",
    "y_train = Preprocessing.load_data(os.path.join(root_directory, \"data\", \"y_train.csv\"))['y']\n",
    "\n",
    "\n",
    "feature_names = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCk6-RiwPK8V",
    "outputId": "8c6949e9-e621-401a-e748-53cbbe9c01e1"
   },
   "outputs": [],
   "source": [
    "# Imputation and Outlier detection\n",
    "X_train, y_train = preprocessing.simple_imputation_and_outlier_ECOD(X_train, feature_names, y_train, fit_transformers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-OjzgrO-PubS"
   },
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "X_train, y_train = preprocessing.scale_data(X_train, y_train, fit_transformers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vPuPFR9KQMdA"
   },
   "outputs": [],
   "source": [
    "# Performing feature selection\n",
    "X_train_updated, y_train_updated = preprocessing.feature_selection(X_train,y_train, fit_transformers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_filename = \"Prediction_of_Brain_Age.ipynb\"\n",
    "\n",
    "# Get the directory of the current notebook file\n",
    "notebook_directory = os.path.dirname(os.path.abspath(notebook_filename))\n",
    "\n",
    "# Get the parent directory (main folder) of the notebook directory\n",
    "main_folder_path = os.path.dirname(notebook_directory)\n",
    "\n",
    "X_train_updated_path = os.path.join(main_folder_path, 'data', 'X_train_updated.csv')\n",
    "y_train_updated_path = os.path.join(main_folder_path, 'data', 'y_train_updated.csv')\n",
    "\n",
    "\n",
    "pd.DataFrame(X_train_updated).to_csv(X_train_updated_path, index=False)\n",
    "y_train_updated.to_csv(y_train_updated_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfBLFqQ4UbtS",
    "outputId": "c0ca7d96-cc3d-46be-a163-86b014e7b3f6"
   },
   "outputs": [],
   "source": [
    "# Perform cross validation and Grid Search to see the best hyperparameters\n",
    "from hyperparameters import SVR_PARAMS\n",
    "\n",
    "\n",
    "\n",
    "svr_model = SVR()\n",
    "svr = Training(svr_model, SVR_PARAMS)\n",
    "svr.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfBLFqQ4UbtS",
    "outputId": "c0ca7d96-cc3d-46be-a163-86b014e7b3f6"
   },
   "outputs": [],
   "source": [
    "# Perform cross validation and Grid Search to see the best hyperparameters\n",
    "from hyperparameters import RFR_PARAMS\n",
    "\n",
    "\n",
    "rfr_model=RandomForestRegressor()\n",
    "rfr = Training(rfr_model, RFR_PARAMS)\n",
    "rfr.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import GB_PARAMS\n",
    "\n",
    "\n",
    "gb_model=GradientBoostingRegressor()\n",
    "gb = Training(gb_model, GB_PARAMS)\n",
    "gb.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import DTR_PARAMS\n",
    "\n",
    "dtr_model = DecisionTreeRegressor()\n",
    "dtr = Training(dtr_model, DTR_PARAMS)\n",
    "dtr.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_PARAMS = {\n",
    " 'n_estimators': [50, 100, 500],\n",
    " 'learning_rate': [0.001, 0.01, 0.1],\n",
    " 'loss': ['linear', 'square', 'exponential'],\n",
    " 'random_state': [42]}\n",
    "\n",
    "ab_model = AdaBoostRegressor()\n",
    "\n",
    "# Create an instance of the Training class\n",
    "ab = Training(ab_model, param_grid=AB_PARAMS)\n",
    "\n",
    "# Assuming X_train_updated and y_train_updated are defined elsewhere\n",
    "ab.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import GPR_PARAMS\n",
    "\n",
    "gpr_model = GaussianProcessRegressor()\n",
    "gpr = Training(gpr_model, GPR_PARAMS)\n",
    "gpr.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import XGB_PARAMS\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb = Training(xgb_model, XGB_PARAMS)\n",
    "xgb.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import ET_PARAMS\n",
    "\n",
    "et_model = ExtraTreesRegressor()\n",
    "et = Training(et_model, ET_PARAMS)\n",
    "et.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperparameters import RIDGE_PARAMS\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge = Training(ridge_model, RIDGE_PARAMS)\n",
    "ridge.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperparameters import ELASTICNET_PARAMS\n",
    "\n",
    "elasticnet_model = ElasticNet()\n",
    "elasticnet = Training(elasticnet_model, ELASTICNET_PARAMS)\n",
    "elasticnet.find_best_parameters(X_train_updated, y_train_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the stacking regressor to achieve the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Categorical\n",
    "from tuned_models import svr, rfr, gb, dtr, ab, gpr, xgb, et, ridge, elasticnet\n",
    "from preprocessing import Preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "\n",
    "\n",
    "\n",
    "# Define base_estimators_pool dictionary\n",
    "base_estimators_pool = {\n",
    "    'svr': svr,\n",
    "    'rf': rfr,\n",
    "    'gb': gb,\n",
    "    'dt': dtr,\n",
    "    'ab': ab,\n",
    "    'et': et,\n",
    "    'xg': xgb,\n",
    "    'gpr': gpr,\n",
    "    'en': elasticnet,\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for optimizing a stacking regressor.\n",
    "    \"\"\"\n",
    "  \n",
    "    base_estimators = [(name, estimator) for name, use, estimator in zip(base_estimators_pool.keys(), params, base_estimators_pool.values()) if use]\n",
    "\n",
    "    # Define the stacking regressor\n",
    "    stack = StackingRegressor(estimators=base_estimators, final_estimator=ridge)\n",
    "\n",
    "    # Compute the cross-validated score\n",
    "    # Compute the cross-validated R^2 score\n",
    "    score = cross_val_score(stack, X_train_updated, y_train_updated, cv=5, scoring='r2', n_jobs=-1)\n",
    "    return -np.mean(score)  # Negate the score because we want to maximize R^2\n",
    "\n",
    "# Define the search space dimensions\n",
    "search_space = [Categorical([True, False]) for _ in base_estimators_pool]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "result = gp_minimize(objective, search_space, n_calls=50, n_initial_points=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Interpret the result\n",
    "best_combination = [name for name, use in zip(base_estimators_pool.keys(), result.x) if use]\n",
    "\n",
    "print(f\"Best combination of base estimators: {best_combination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the parameters of the kernels used in GPR to achieve the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to minimize: Negative R-squared score of GPR with Rational Quadratic kernel\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, Matern, WhiteKernel\n",
    "\n",
    "# For objective gaussian\n",
    "def objective_gaussian(params):\n",
    "    \"\"\"\n",
    "    Objective function for optimizing a Gaussian process regressor.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : array-like\n",
    "        Array-like object containing the hyperparameters to be optimized.\n",
    "        The elements of params are in the order: constant_value, length_scale, alpha.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Negative mean of the cross-validated R^2 scores of the Gaussian process regressor.\n",
    "        The negative value is returned because optimization algorithms typically \n",
    "        seek to minimize the objective function, whereas we want to maximize R^2.\n",
    "    \"\"\"\n",
    "     \n",
    "    constant_value, length_scale, alpha = params\n",
    "    kernel = ConstantKernel(constant_value=constant_value**2) * RationalQuadratic(length_scale=length_scale, alpha=alpha)\n",
    "    model = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "    # Negate the R^2 score because gp_minimize seeks to minimize the objective\n",
    "    return -np.mean(cross_val_score(model, X_train_updated, y_train_updated, cv=5, scoring='r2', n_jobs=-1))\n",
    "\n",
    "# Define the bounds of the search space\n",
    "search_space = [\n",
    "    Real(0.1, 2.0, name='constant_value'),  # Scaling factor\n",
    "    Real(0.1, 10.0, name='length_scale'),   # Kernel length scale\n",
    "    Real(0.1, 10.0, name='alpha')           # Kernel alpha\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "result = gp_minimize(objective_gaussian, search_space, n_calls=200, n_initial_points=25, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Best parameters found\n",
    "best_constant_value = result.x[0]\n",
    "best_length_scale = result.x[1]\n",
    "best_alpha = result.x[2]\n",
    "best_kernel = ConstantKernel(constant_value=best_constant_value**2) * RationalQuadratic(length_scale=best_length_scale, alpha=best_alpha)\n",
    "print(f\"Best kernel: {best_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating StackingRegressor for performing predictions on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tuned_models import svr, rfr, gb, dtr, ab, gpr, xgb, et, ridge, elasticnet\n",
    "\n",
    "estimators = [\n",
    "            ('svr', svr),\n",
    "            ('rf', rfr),\n",
    "            ('gb', gb),\n",
    "            ('dt', dtr),\n",
    "            ('ab', ab),\n",
    "            ('et', et),\n",
    "            ('xg', xgb),\n",
    "            #('lgbm', lgbm),\n",
    "            ('gpr', gpr),\n",
    "            #('cb', cb),\n",
    "            #('rvr', rvr),\n",
    "            ('en', elasticnet),\n",
    "            # ('ridge', ridge)\n",
    "]\n",
    "\n",
    "stack1 = StackingRegressor(\n",
    "    estimators=estimators[0:4],\n",
    "    final_estimator=gpr,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack2 = StackingRegressor(\n",
    "    estimators=estimators[4:],\n",
    "    final_estimator=gpr,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack_final = StackingRegressor(\n",
    "    estimators=[(\"stack1\",stack1),(\"stack2\",stack2)],\n",
    "    final_estimator=gpr,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the datasets and performing predictions\n",
    "from predict import Prediction\n",
    "from train import Training\n",
    "\n",
    "train_final = Training(stack_final)\n",
    "prediction = Prediction(train_final)\n",
    "\n",
    "\n",
    "#Load the test set\n",
    "data_directory = './data'\n",
    "X_test = os.path.join(data_directory, 'X_test.csv')\n",
    "# Generating predictions\n",
    "\n",
    "y_pred = prediction.generate_predictions(X_train, y_train, X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_pred_final = prediction.resize(scaler_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "\n",
    "#notebook_filename = \"Prediction_of_Brain_Age.ipynb\"\n",
    "\n",
    "# Get the directory of the current notebook file\n",
    "#notebook_directory = os.path.dirname(os.path.abspath(notebook_filename))\n",
    "\n",
    "# Get the parent directory (main folder) of the notebook directory\n",
    "#main_folder_path = os.path.dirname(notebook_directory)\n",
    "# Create DataFrame\n",
    "df_final = pd.DataFrame({'id': np.arange(len(y_pred_final)), 'y': y_pred_final.flatten()})\n",
    "\n",
    "# Get the current directory\n",
    "\n",
    "# Adjust file path for Windows\n",
    "csv_file_path = os.path.join(main_folder_path, \"data\", \"y_pred_stackdoublestack_2.csv\")\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_final.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"CSV file saved successfully at:\", csv_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
